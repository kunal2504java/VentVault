# VentVault Backend

Privacy-first, high-performance backend for emotional venting platform.

## ðŸŽ¯ Architecture Principles

- **Stateless hot path** - No DB writes during vent
- **Streaming LLM** - Time-to-first-token optimized
- **Anonymous-first** - No auth required for core experience
- **Privacy-by-design** - No content logging, PII scrubbing

## ðŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Redis (local or cloud)
- OpenAI or Anthropic API key

### Installation

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Setup environment
cp .env.example .env
# Edit .env with your API keys
```

### Run Development Server

```bash
# Start Redis (if local)
redis-server

# Run FastAPI
python -m app.main
```

Server runs at: `http://localhost:8000`

API docs: `http://localhost:8000/docs`

## ðŸ“¡ API Endpoints

### POST /api/vent

Core venting endpoint - streams LLM response.

**Request:**
```json
{
  "mode": "text",
  "content": "I feel overwhelmed and exhausted"
}
```

**Response:** Server-sent events stream

**Headers:**
- `X-Session-ID` - Anonymous session identifier
- `X-Remaining-Vents` - Vents remaining today

**Rate Limits:**
- Anonymous: 2 vents/day
- Signed-in: 10 vents/day

## ðŸ”’ Privacy Features

- âœ… PII scrubbing (emails, phones, URLs)
- âœ… No content logging
- âœ… No raw text storage
- âœ… Anonymous device hashing
- âœ… Metadata-only persistence (future)

## âš¡ Performance Targets

| Metric | Target | Current |
|--------|--------|---------|
| P95 latency | < 2s | TBD |
| Time-to-first-token | < 600ms | TBD |
| API logic | < 100ms | TBD |

## ðŸ› ï¸ Tech Stack

- **Framework:** FastAPI
- **LLM:** OpenAI GPT-4o-mini / Anthropic Claude Haiku
- **Cache:** Redis
- **Async:** Native Python asyncio

## ðŸ“¦ Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI app + routes
â”‚   â”œâ”€â”€ config.py            # Settings management
â”‚   â”œâ”€â”€ models.py            # Pydantic models
â”‚   â”œâ”€â”€ pii_scrubber.py      # PII removal
â”‚   â”œâ”€â”€ rate_limiter.py      # Redis-based rate limiting
â”‚   â””â”€â”€ llm_service.py       # Streaming LLM client
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ðŸ”® Roadmap

### Phase 1 (Current)
- [x] Core vent API
- [x] PII scrubbing
- [x] Rate limiting
- [x] Streaming LLM

### Phase 2 (Next)
- [ ] Async background worker
- [ ] Emotion classification
- [ ] Metadata storage
- [ ] Mood map generation

### Phase 3 (Future)
- [ ] Auth integration
- [ ] User accounts
- [ ] Premium features
- [ ] Analytics dashboard

## ðŸ§ª Testing

```bash
# Run tests (when added)
pytest

# Load testing
# locust -f tests/load_test.py
```

## ðŸš¢ Deployment

### Docker

```bash
docker build -t ventvault-backend .
docker run -p 8000:8000 ventvault-backend
```

### Fly.io / Railway / AWS

See deployment guides in `/docs` (coming soon)

## ðŸ“Š Monitoring

Key metrics to track:
- Request latency (P50, P95, P99)
- LLM time-to-first-token
- Rate limit hits
- Error rates
- Cost per vent

## ðŸ¤ Contributing

This is a focused V1 implementation. See `IMPLEMENTATION_PLAN.md` for full architecture.

## ðŸ“„ License

Proprietary - VentVault
